{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4v8F91nziLJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "# Libraries we need\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Sequential, Model,load_model\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
        "from keras.preprocessing import image\n",
        "from keras.initializers import glorot_uniform\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D,Dropout\n",
        "import tensorflow as tf\n",
        "import splitfolders \n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "## getting our data file\n",
        "data_dir = \"../input/pistachio-image-dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset/\"\n",
        "data_dir =pathlib.Path(data_dir)\n",
        "\n",
        "\n",
        "\n",
        "Total_Images = glob.glob('../input/pistachio-image-dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset/*/*.jpg')\n",
        "print(\"Total number of images: \", len(Total_Images))\n",
        "Total_Images = pd.Series(Total_Images)\n",
        "print(Total_Images)\n",
        "\n",
        "total_df = pd.DataFrame()\n",
        "\n",
        "# generate Filename field\n",
        "total_df['Filename'] = Total_Images.map( lambda img_name: img_name.split(\"/\")[-1])\n",
        "\n",
        "print(total_df['Filename'])\n",
        "# generate ClassId field\n",
        "total_df['ClassId'] = Total_Images.map(lambda img_name: img_name.split(\"/\")[-2])\n",
        "\n",
        "total_df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "splitfolders.ratio(data_dir, output=\"output\", seed=101, ratio=(.8, .1, .1))\n",
        "\n",
        "\n",
        "dir_list = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "# Print the names of the directories\n",
        "for directory in dir_list:\n",
        "    print(directory)\n",
        "train_path='./output/train/'\n",
        "val_path='./output/val'\n",
        "test_path='./output/test'\n",
        "class_names=os.listdir(train_path)\n",
        "class_names_val=os.listdir(val_path)\n",
        "class_names_test=os.listdir(test_path)\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "\n",
        "train_image1 = glob.glob('./output/train/*/*.jpg')\n",
        "Total_TrainImages = train_image1 \n",
        "print(\"Total number of training images: \", len(Total_TrainImages))\n",
        "\n",
        "\n",
        "test_image1 = glob.glob('./output/test/*/*.jpg')\n",
        "Total_TestImages = test_image1 \n",
        "print(\"Total number of test images: \", len(Total_TestImages))\n",
        "\n",
        "\n",
        "\n",
        "Val_image1 = glob.glob('./output/val/*/*.jpg')\n",
        "\n",
        "Total_ValImages = Val_image1 \n",
        "print(\"Total number of val images: \", len(Total_ValImages))\n",
        "\n",
        "\n",
        "train_image_names = pd.Series(Total_TrainImages)\n",
        "train_df = pd.DataFrame()\n",
        "\n",
        "# generate Filename field\n",
        "train_df['Filename'] = train_image_names.map( lambda img_name: img_name.split(\"/\")[-1])\n",
        "\n",
        "\n",
        "# generate ClassId field\n",
        "train_df['ClassId'] = train_image_names.map(lambda img_name: img_name.split(\"/\")[-2])\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "class_id_distribution_Train = train_df['ClassId'].value_counts()\n",
        "class_id_distribution_Train.head(10)\n",
        "\n",
        "\n",
        "test_image_names = pd.Series(Total_TestImages)\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "# generate Filename field\n",
        "test_df['Filename'] = test_image_names.map( lambda img_name: img_name.split(\"/\")[-1])\n",
        "\n",
        "\n",
        "# generate ClassId field\n",
        "test_df['ClassId'] = test_image_names.map(lambda img_name: img_name.split(\"/\")[-2])\n",
        "\n",
        "test_df.head()\n",
        "\n",
        "\n",
        "class_id_distribution_test = test_df['ClassId'].value_counts()\n",
        "class_id_distribution_test.head(10)\n",
        "\n",
        "#display some of the images\n",
        "plot_df = train_df.sample(12).reset_index()\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "for i in range(12):\n",
        "    img_name = plot_df.loc[i, 'Filename']\n",
        "    label_str = (plot_df.loc[i, 'ClassId'])\n",
        "    plt.subplot(4,4,i+1)\n",
        "    a=os.path.join(train_path,label_str, img_name)\n",
        "    plt.imshow(plt.imread(os.path.join(train_path,label_str, img_name)))\n",
        "    plt.title(label_str)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "\n",
        "\n",
        "## Image Generator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15)\n",
        "test_datagen = ImageDataGenerator()\n",
        "val_datagen = ImageDataGenerator()\n",
        "train_generator = train_datagen.flow_from_directory(train_path,target_size=(224, 224),batch_size=32,shuffle=True,class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(test_path,target_size=(224,224),batch_size=32,shuffle=False,class_mode='binary')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(val_path,target_size=(224,224),batch_size=32,shuffle=False,class_mode='binary')\n",
        "\n",
        "\n",
        "# Efficent Net\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "model = EfficientNetB0(\n",
        "      input_shape = (224,224,3),\n",
        "      include_top = False,\n",
        "      weights = 'imagenet'\n",
        "    )\n",
        "\n",
        "\n",
        "# Fine tuning\n",
        "model.trainable = True\n",
        "for layer in model.layers[:-15]:\n",
        "    layer.trainable=False\n",
        "\n",
        "def print_layer_trainable():\n",
        "    for layer in model.layers:\n",
        "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n",
        "print_layer_trainable()\n",
        "\n",
        "\n",
        "#Adding some layer from our side\n",
        "from keras.layers import  Dropout\n",
        "x = Flatten()(model.output)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation = \"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(model.input, x)\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n",
        "\n",
        "##Ploting the model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "plot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\n",
        "Image(filename='convnet.png') \n",
        "\n",
        "\n",
        "## call back\n",
        "es=EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
        "\n",
        "mc = ModelCheckpoint('model.h5', monitor='val_accuracy', mode='max',save_best_only=True )\n",
        "\n",
        "## lets train the model\n",
        "H = model.fit(train_generator,validation_data=val_generator,epochs=50,verbose=1,callbacks=[mc,es])\n",
        "\n",
        "\n",
        "\n",
        "# plotting the output\n",
        "acc = H.history['accuracy']\n",
        "val_acc = H.history['val_accuracy']\n",
        "loss = H.history['loss']\n",
        "val_loss = H.history['val_loss']\n",
        "\n",
        "\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test set\n",
        " test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "\n",
        " # Classification Report\n",
        " from sklearn.metrics import classification_report\n",
        "\n",
        " print(classification_report(y_val,y_pred))"
      ]
    }
  ]
}